{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Tarea_v03.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "living-philosophy",
        "numerous-invite",
        "optical-arizona",
        "sweet-implement",
        "present-consortium"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniacgm/Deep_Learning/blob/cnn/Tarea_v03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prospective-america"
      },
      "source": [
        "Dado que el entrenamiento de redes neuronales es una tarea  muy costosa, **se recomienda ejecutar el notebooks en [Google Colab](https://colab.research.google.com)**, por supuesto también se puede ejecutar en local.\n",
        "\n",
        "Al entrar en [Google Colab](https://colab.research.google.com) bastará con hacer click en `upload` y subir este notebook. No olvide luego descargarlo en `File->Download .ipynb`\n",
        "\n",
        "**El examen deberá ser entregado con las celdas ejecutadas, si alguna celda no está ejecutadas no se contará.**"
      ],
      "id": "prospective-america"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "novel-stewart"
      },
      "source": [
        "El examen se divide en tres partes, con la puntuación que se indica a continuación. La puntuación máxima será 10.\n",
        "\n",
        "- [Actividad 1: Redes Densas](#actividad_1): 5.5 pts\n",
        "    - Correcta normalización: máximo de 0.5 pts\n",
        "    - [Cuestión 1](#1.1): 1 pt\n",
        "    - [Cuestión 2](#1.2): 1 pt\n",
        "    - [Cuestión 3](#1.3): 0.5 pts\n",
        "    - [Cuestión 4](#1.4): 0.5 pts\n",
        "    - [Cuestión 5](#1.5): 0.5 pts\n",
        "    - [Cuestión 6](#1.6): 0.5 pts\n",
        "    - [Cuestión 7](#1.7): 0.5 pts\n",
        "    - [Cuestión 8](#1.8): 0.5 pts\n",
        "\n",
        "\n",
        "- [Actividad 2: Redes Convolucionales](#actividad_2): 4.5 pts\n",
        "    - [Cuestión 1](#2.1): 1 pt\n",
        "    - [Cuestión 2](#2.2): 1.5 pt\n",
        "    - [Cuestión 3](#2.3): 0.5 pts\n",
        "    - [Cuestión 4](#2.4): 0.5 pts\n",
        "    - [Cuestión 5](#2.5): 0.5 pts\n",
        "    - [Cuestión 6](#2.6): 0.5 pts\n"
      ],
      "id": "novel-stewart"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prompt-developer"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "id": "prompt-developer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vocal-correction"
      },
      "source": [
        "<a name='actividad_1'></a>\n",
        "# Actividad 1: Redes Densas\n",
        "\n",
        "Para esta primera actividad vamos a utilizar el [boston housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). Con el que trataremos de predecir el precio de una casa con 13 features.\n",
        "\n",
        "**Puntuación**: \n",
        "\n",
        "Normalizar las features correctamente (x_train, x_test): 0.5 pts \n",
        "\n",
        "- Correcta normalización: máximo de 0.5 pts\n",
        "- [Cuestión 1](#1.1): 1 pt\n",
        "- [Cuestión 2](#1.2): 1 pt\n",
        "- [Cuestión 3](#1.3): 0.5 pts\n",
        "- [Cuestión 4](#1.4): 0.5 pts\n",
        "- [Cuestión 5](#1.5): 0.5 pts\n",
        "- [Cuestión 6](#1.6): 0.5 pts\n",
        "- [Cuestión 7](#1.7): 0.5 pts\n",
        "- [Cuestión 8](#1.8): 0.5 pts\n",
        "\n"
      ],
      "id": "vocal-correction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "presidential-milan"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path='boston_housing.npz',\n",
        "    test_split=0.2,\n",
        ")\n",
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_train.shape, y_train.shape)\n",
        "print('Some prices: ', y_train[:5])"
      ],
      "id": "presidential-milan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0teFVimjk2t"
      },
      "source": [
        "Normalizamos los datos porque los valores de las variables tienen escala diferente. En cada caso restamos la media y dividimos por la desviación estándar"
      ],
      "id": "-0teFVimjk2t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "painted-extreme"
      },
      "source": [
        "train_mean = np.mean(x_train, axis=0)\n",
        "train_std = np.std(x_train, axis=0)\n",
        "x_train = (x_train - train_mean) / train_std\n",
        "\n",
        "train_mean = np.mean(x_test, axis=0)\n",
        "train_std = np.std(x_test, axis=0)\n",
        "x_test = (x_test - train_mean) / train_std\n",
        "\n",
        "\n"
      ],
      "id": "painted-extreme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "underlying-planner"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "## Cuestión 1: Cree un modelo secuencial que contenga 4 capas ocultas(hidden layers), con más de 60 neuronas  por capa, sin regularización y obtenga los resultados.\n",
        "\n",
        "Puntuación: \n",
        "- Obtener el modelo correcto: 0.8 pts\n",
        "- Compilar el modelo: 0.1pts\n",
        "- Acertar con la función de pérdida: 0.1 pts"
      ],
      "id": "underlying-planner"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVIasehnRMnt"
      },
      "source": [
        "# Fijamos una semilla para asegurar que el resultado es reproducible\n",
        "\n",
        "tf.random.set_seed(1)\n"
      ],
      "id": "TVIasehnRMnt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "working-shade"
      },
      "source": [
        "# Vamos a diseñar la red a través de una API funcional porque el modelo es complejo\n",
        "\n",
        "# definimos la entrada\n",
        "inputs = keras.Input(shape=(13,), name='input_layer')\n",
        "\n",
        "# Ahora definimos las capas ocultas que constituyen la red y la relación entre ellas. Cada capa incluye más de 60 neuronas. Utilizamos la función de \n",
        "# activacion relu porque es la más adecuada para este tipo de problema de tipo regresión\n",
        "\n",
        "layer_1 = layers.Dense (64, activation='relu',\n",
        "                        name = 'layer_1') (inputs)\n",
        "layer_2 = layers.Dense (64, activation='relu',\n",
        "                        name = 'layer_2') (layer_1)\n",
        "layer_3 = layers.Dense (64, activation='relu',\n",
        "                        name = 'layer_3') (layer_2)\n",
        "layer_4 = layers.Dense (64, activation='relu',\n",
        "                        name = 'layer_4') (layer_3)\n",
        "# al final definimos la capa de salida                        \n",
        "outputs = layers.Dense (1, name = 'output_layer') (layer_4) \n",
        "# y creamos el modelo \n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='my_model') \n"
      ],
      "id": "working-shade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXs7K2X6Nf66",
        "outputId": "bfdd413d-de47-4c0b-f0ee-2ae72d492bf4"
      },
      "source": [
        "model.summary()"
      ],
      "id": "IXs7K2X6Nf66",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (InputLayer)     [(None, 13)]              0         \n",
            "_________________________________________________________________\n",
            "layer_1 (Dense)              (None, 64)                896       \n",
            "_________________________________________________________________\n",
            "layer_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "layer_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "layer_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 13,441\n",
            "Trainable params: 13,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mobile-change"
      },
      "source": [
        "# Compilación del modelo\n",
        "# Como se trata de un problema de regresión utilizamos el MAE\n",
        "model.compile(optimizer='adam', \n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])"
      ],
      "id": "mobile-change",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rotary-credits"
      },
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=32,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ],
      "id": "rotary-credits",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "descending-letters",
        "outputId": "42379446-cc33-483b-ae84-c4bf691aa3a5"
      },
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ],
      "id": "descending-letters",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 18.9684 - mae: 2.7377\n",
            "Test Loss: [18.96837615966797, 2.737718105316162]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raised-delivery"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "## Cuestión 2: Utilice el mismo modelo de la cuestión anterior pero añadiendo al menos dos técnicas distinas de regularización.\n",
        "\n",
        "Ejemplos de regularización: [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb)\n",
        "\n",
        "Puntuación:\n",
        "\n",
        "- Obtener el modelo con la regularización: 0.8 pts\n",
        "- Obtener un `test loss` inferior al anterior: 0.2 pts\n"
      ],
      "id": "raised-delivery"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KElQ8p_h_pjk"
      },
      "source": [
        "**REGULARIZACIÓN TIPO 1. REDUCIR EL BATCH_SIZE**"
      ],
      "id": "KElQ8p_h_pjk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uVX0vshSKT9"
      },
      "source": [
        "# Fijamos una semilla para asegurar que el resultado es reproducible\n",
        "\n",
        "tf.random.set_seed(1)\n"
      ],
      "id": "-uVX0vshSKT9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTaasBil_3xu"
      },
      "source": [
        "# Creamos el modelo\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "inputs = keras.Input(shape=(13,), name='input_layer')\n",
        "\n",
        "layer_1 = layers.Dense (64, activation='relu', name = 'layer_1') (inputs)\n",
        "layer_2 = layers.Dense (64, activation='relu', name = 'layer_2') (layer_1)\n",
        "layer_3 = layers.Dense (64, activation='relu', name = 'layer_3') (layer_2)\n",
        "layer_4 = layers.Dense (64, activation='relu', name = 'layer_4') (layer_3)\n",
        "                       \n",
        "outputs = layers.Dense (1, name = 'output_layer') (layer_4) \n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='my_model') "
      ],
      "id": "JTaasBil_3xu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rwa_wfi_4DX"
      },
      "source": [
        "# Compilamos\n",
        "model.compile(optimizer='adam', \n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])"
      ],
      "id": "3Rwa_wfi_4DX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oj4yivGAT2P"
      },
      "source": [
        "# Establecemos un batch_size de 16 para ver qué impacto tiene una reducción de este parámetro como método de regulación, que en el ejercicio anterior era de 32\n",
        "\n",
        "batch_size=16"
      ],
      "id": "0Oj4yivGAT2P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgol4wLQAXRO",
        "outputId": "cd8e88db-2c12-4ddc-a396-1d8379fba52f"
      },
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ],
      "id": "Wgol4wLQAXRO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 12ms/step - loss: 540.8517 - mae: 21.4170 - val_loss: 523.5873 - val_mae: 20.8720\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 377.4734 - mae: 17.1820 - val_loss: 126.5977 - val_mae: 8.2211\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 87.8150 - mae: 7.1112 - val_loss: 55.2201 - val_mae: 5.3547\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 41.0133 - mae: 4.5732 - val_loss: 28.8950 - val_mae: 3.9603\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 20.4728 - mae: 3.1966 - val_loss: 22.3934 - val_mae: 3.6285\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 20.7162 - mae: 3.1828 - val_loss: 18.8070 - val_mae: 3.2651\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 15.2096 - mae: 2.8245 - val_loss: 17.6060 - val_mae: 3.2310\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.1529 - mae: 2.5392 - val_loss: 16.2256 - val_mae: 3.1043\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.0097 - mae: 2.4900 - val_loss: 15.6200 - val_mae: 3.0535\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.0399 - mae: 2.5655 - val_loss: 15.8177 - val_mae: 3.0989\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 15.4009 - mae: 2.6044 - val_loss: 15.3105 - val_mae: 3.0515\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 13.1576 - mae: 2.4494 - val_loss: 14.6881 - val_mae: 2.9107\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.9088 - mae: 2.3988 - val_loss: 14.6972 - val_mae: 2.9153\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.6082 - mae: 2.3897 - val_loss: 14.8554 - val_mae: 2.9811\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 11.3172 - mae: 2.3504 - val_loss: 15.1063 - val_mae: 2.8881\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 8.7856 - mae: 2.1072 - val_loss: 15.2158 - val_mae: 2.9478\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.3383 - mae: 2.2591 - val_loss: 15.1110 - val_mae: 2.9442\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.7087 - mae: 1.8796 - val_loss: 15.7147 - val_mae: 2.8495\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.3422 - mae: 2.2668 - val_loss: 15.1798 - val_mae: 2.9093\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.4134 - mae: 2.1201 - val_loss: 14.6616 - val_mae: 2.8316\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.8040 - mae: 1.9300 - val_loss: 15.0860 - val_mae: 2.8915\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.2727 - mae: 2.1958 - val_loss: 15.4289 - val_mae: 2.8550\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.6174 - mae: 1.9926 - val_loss: 14.9283 - val_mae: 2.7864\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.2059 - mae: 2.0485 - val_loss: 16.2475 - val_mae: 2.8988\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.3281 - mae: 2.0193 - val_loss: 16.6955 - val_mae: 2.9577\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.4396 - mae: 2.0070 - val_loss: 15.1588 - val_mae: 2.9503\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.9962 - mae: 2.0048 - val_loss: 15.2626 - val_mae: 2.8114\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 8.0088 - mae: 2.0112 - val_loss: 14.9426 - val_mae: 2.8792\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.8141 - mae: 1.7943 - val_loss: 16.2966 - val_mae: 2.7985\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.5682 - mae: 1.8718 - val_loss: 15.0221 - val_mae: 2.8309\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.8429 - mae: 1.7947 - val_loss: 15.0082 - val_mae: 2.8192\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.0096 - mae: 1.7696 - val_loss: 15.6656 - val_mae: 2.7554\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.0231 - mae: 1.8272 - val_loss: 16.9058 - val_mae: 2.8726\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 7.7071 - mae: 2.0905 - val_loss: 23.9385 - val_mae: 3.3230\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.2127 - mae: 2.0984 - val_loss: 17.5906 - val_mae: 2.9110\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.0395 - mae: 1.9269 - val_loss: 15.7144 - val_mae: 2.8515\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 5.6789 - mae: 1.7707 - val_loss: 14.8988 - val_mae: 2.7155\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.3387 - mae: 1.8689 - val_loss: 15.0983 - val_mae: 2.8526\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.6110 - mae: 1.7076 - val_loss: 14.5959 - val_mae: 2.7959\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.3749 - mae: 1.6441 - val_loss: 14.9730 - val_mae: 2.6293\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.6864 - mae: 1.7949 - val_loss: 13.9539 - val_mae: 2.7711\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.1652 - mae: 1.6035 - val_loss: 14.6378 - val_mae: 2.6942\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.7609 - mae: 1.6696 - val_loss: 14.6518 - val_mae: 2.7570\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.4477 - mae: 1.7268 - val_loss: 17.4381 - val_mae: 2.7701\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.0349 - mae: 1.8356 - val_loss: 15.5502 - val_mae: 2.7058\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.7219 - mae: 1.6156 - val_loss: 14.9169 - val_mae: 2.7104\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 11ms/step - loss: 6.3232 - mae: 1.7273 - val_loss: 14.7223 - val_mae: 2.6618\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.9958 - mae: 1.5775 - val_loss: 15.5605 - val_mae: 2.8050\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.2441 - mae: 1.8106 - val_loss: 15.7229 - val_mae: 2.8448\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.1009 - mae: 1.8229 - val_loss: 16.1485 - val_mae: 2.7438\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.6460 - mae: 1.6063 - val_loss: 14.6495 - val_mae: 2.6388\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.3516 - mae: 1.5680 - val_loss: 13.8949 - val_mae: 2.6403\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.0259 - mae: 1.5466 - val_loss: 15.9061 - val_mae: 2.7652\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.4462 - mae: 1.5438 - val_loss: 15.9298 - val_mae: 2.7518\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.5989 - mae: 1.6306 - val_loss: 13.6658 - val_mae: 2.5506\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.2675 - mae: 1.5287 - val_loss: 16.8663 - val_mae: 2.7735\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.7829 - mae: 1.6932 - val_loss: 14.1242 - val_mae: 2.5601\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 4.0792 - mae: 1.4585 - val_loss: 14.9210 - val_mae: 2.6060\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.2683 - mae: 1.3052 - val_loss: 14.5335 - val_mae: 2.6094\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 3.8003 - mae: 1.4521 - val_loss: 14.3799 - val_mae: 2.6688\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.6292 - mae: 1.4120 - val_loss: 14.6089 - val_mae: 2.6163\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.9171 - mae: 1.3448 - val_loss: 14.8310 - val_mae: 2.7586\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.0185 - mae: 1.4750 - val_loss: 13.6220 - val_mae: 2.5597\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.0923 - mae: 1.3107 - val_loss: 14.6657 - val_mae: 2.5629\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.9931 - mae: 1.4195 - val_loss: 14.3890 - val_mae: 2.6270\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.7466 - mae: 1.4522 - val_loss: 14.0046 - val_mae: 2.5707\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.6381 - mae: 1.1196 - val_loss: 15.4426 - val_mae: 2.5995\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 5.0165 - mae: 1.6407 - val_loss: 14.7180 - val_mae: 2.5285\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.9139 - mae: 1.6652 - val_loss: 13.9216 - val_mae: 2.6026\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.1793 - mae: 1.3868 - val_loss: 14.9579 - val_mae: 2.5998\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.2683 - mae: 1.5593 - val_loss: 14.3405 - val_mae: 2.5682\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.9437 - mae: 1.2753 - val_loss: 15.4529 - val_mae: 2.5461\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.4390 - mae: 1.2890 - val_loss: 13.8277 - val_mae: 2.5261\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.0470 - mae: 1.2475 - val_loss: 16.0302 - val_mae: 2.5999\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.2275 - mae: 1.2841 - val_loss: 14.8140 - val_mae: 2.5463\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.3515 - mae: 1.2896 - val_loss: 13.7725 - val_mae: 2.5232\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.9391 - mae: 1.2653 - val_loss: 14.4149 - val_mae: 2.5375\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.5236 - mae: 1.2467 - val_loss: 14.1828 - val_mae: 2.5317\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.9438 - mae: 1.2139 - val_loss: 13.8812 - val_mae: 2.5977\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.8732 - mae: 1.2627 - val_loss: 15.0905 - val_mae: 2.5678\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.8279 - mae: 1.1701 - val_loss: 18.3242 - val_mae: 2.7712\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.5084 - mae: 1.4079 - val_loss: 14.4570 - val_mae: 2.5341\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.0924 - mae: 1.3365 - val_loss: 15.9138 - val_mae: 2.6273\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.5845 - mae: 1.1576 - val_loss: 14.6046 - val_mae: 2.5363\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.3546 - mae: 1.1276 - val_loss: 14.6214 - val_mae: 2.5064\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.1936 - mae: 1.2763 - val_loss: 14.8818 - val_mae: 2.5959\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.3021 - mae: 1.5621 - val_loss: 16.2060 - val_mae: 2.6546\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 3.5288 - mae: 1.3288 - val_loss: 12.7931 - val_mae: 2.4501\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.3163 - mae: 1.2239 - val_loss: 12.5248 - val_mae: 2.4635\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.7798 - mae: 1.1777 - val_loss: 14.0113 - val_mae: 2.5214\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.5430 - mae: 1.3279 - val_loss: 14.7730 - val_mae: 2.6104\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.5662 - mae: 1.1561 - val_loss: 13.8726 - val_mae: 2.4937\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.0072 - mae: 1.2265 - val_loss: 13.7688 - val_mae: 2.4862\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.6951 - mae: 1.1033 - val_loss: 14.6325 - val_mae: 2.5911\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.9421 - mae: 1.2581 - val_loss: 13.3973 - val_mae: 2.5003\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1815 - mae: 1.0787 - val_loss: 13.3966 - val_mae: 2.4229\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1307 - mae: 1.0568 - val_loss: 13.9613 - val_mae: 2.4789\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.9563 - mae: 1.0328 - val_loss: 14.8723 - val_mae: 2.5260\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2166 - mae: 1.0836 - val_loss: 13.9646 - val_mae: 2.4820\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.3905 - mae: 1.1170 - val_loss: 14.1533 - val_mae: 2.5394\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2905 - mae: 1.1438 - val_loss: 14.2247 - val_mae: 2.4726\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1171 - mae: 1.0792 - val_loss: 12.5047 - val_mae: 2.4168\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.6265 - mae: 1.1751 - val_loss: 13.5128 - val_mae: 2.4735\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1809 - mae: 1.0643 - val_loss: 13.2922 - val_mae: 2.4632\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.6687 - mae: 0.9556 - val_loss: 13.3397 - val_mae: 2.5077\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2464 - mae: 1.0721 - val_loss: 13.6183 - val_mae: 2.5013\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.3014 - mae: 1.1027 - val_loss: 13.4166 - val_mae: 2.4570\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1385 - mae: 1.0552 - val_loss: 14.2983 - val_mae: 2.5240\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0555 - mae: 1.0651 - val_loss: 14.9445 - val_mae: 2.5404\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.9539 - mae: 1.0363 - val_loss: 14.5526 - val_mae: 2.5555\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0578 - mae: 1.0793 - val_loss: 13.7099 - val_mae: 2.5979\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.5651 - mae: 1.1570 - val_loss: 12.2387 - val_mae: 2.4347\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2646 - mae: 1.1148 - val_loss: 14.4485 - val_mae: 2.6455\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.4776 - mae: 1.1550 - val_loss: 12.3893 - val_mae: 2.4064\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0375 - mae: 1.0301 - val_loss: 13.4174 - val_mae: 2.5152\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1506 - mae: 1.0786 - val_loss: 12.0064 - val_mae: 2.4244\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.5010 - mae: 1.0873 - val_loss: 12.4748 - val_mae: 2.4325\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 2.4214 - mae: 1.1302 - val_loss: 14.8278 - val_mae: 2.5684\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.8760 - mae: 0.9064 - val_loss: 14.3576 - val_mae: 2.5617\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.9090 - mae: 1.0036 - val_loss: 13.9697 - val_mae: 2.5104\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.3268 - mae: 1.1173 - val_loss: 11.3163 - val_mae: 2.3010\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 2.6975 - mae: 1.2044 - val_loss: 12.0597 - val_mae: 2.4320\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2139 - mae: 1.0594 - val_loss: 15.0782 - val_mae: 2.6462\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.3414 - mae: 1.1714 - val_loss: 12.1427 - val_mae: 2.4414\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2373 - mae: 1.0786 - val_loss: 12.8139 - val_mae: 2.4258\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1366 - mae: 1.1014 - val_loss: 12.6365 - val_mae: 2.4570\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1128 - mae: 1.0217 - val_loss: 13.1593 - val_mae: 2.4681\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7902 - mae: 0.9661 - val_loss: 13.3014 - val_mae: 2.5015\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0628 - mae: 1.0268 - val_loss: 13.7817 - val_mae: 2.6398\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.8689 - mae: 1.0151 - val_loss: 13.4363 - val_mae: 2.5059\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0833 - mae: 1.0276 - val_loss: 12.8062 - val_mae: 2.4570\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2363 - mae: 1.1063 - val_loss: 14.2763 - val_mae: 2.6493\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.2164 - mae: 1.3634 - val_loss: 13.6366 - val_mae: 2.5670\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.7005 - mae: 1.2043 - val_loss: 12.8703 - val_mae: 2.4152\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.6044 - mae: 0.9043 - val_loss: 12.2673 - val_mae: 2.4300\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.9618 - mae: 0.9754 - val_loss: 14.0061 - val_mae: 2.5579\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.8069 - mae: 1.0025 - val_loss: 13.6485 - val_mae: 2.4743\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7219 - mae: 0.9544 - val_loss: 12.5807 - val_mae: 2.4586\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.4228 - mae: 0.8999 - val_loss: 13.4116 - val_mae: 2.4371\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.8720 - mae: 0.9456 - val_loss: 14.3700 - val_mae: 2.4954\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.4825 - mae: 0.9238 - val_loss: 13.8495 - val_mae: 2.4710\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7251 - mae: 0.9657 - val_loss: 13.0120 - val_mae: 2.3918\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2349 - mae: 0.8228 - val_loss: 13.8414 - val_mae: 2.5039\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7609 - mae: 0.9344 - val_loss: 13.0246 - val_mae: 2.4063\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7105 - mae: 0.9250 - val_loss: 13.0665 - val_mae: 2.4466\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.6120 - mae: 0.9087 - val_loss: 13.1089 - val_mae: 2.4222\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.6059 - mae: 0.9039 - val_loss: 13.9644 - val_mae: 2.5210\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.4735 - mae: 0.8320 - val_loss: 15.6546 - val_mae: 2.5788\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1642 - mae: 1.1694 - val_loss: 13.8167 - val_mae: 2.5185\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0912 - mae: 1.0346 - val_loss: 12.5774 - val_mae: 2.4266\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.8857 - mae: 0.9599 - val_loss: 13.1909 - val_mae: 2.4391\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.8980 - mae: 1.0170 - val_loss: 13.0617 - val_mae: 2.4361\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.9339 - mae: 1.0195 - val_loss: 13.0377 - val_mae: 2.4970\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.3558 - mae: 1.1634 - val_loss: 13.5989 - val_mae: 2.4809\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7035 - mae: 0.9676 - val_loss: 15.1194 - val_mae: 2.6053\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5722 - mae: 0.9289 - val_loss: 13.4596 - val_mae: 2.4802\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5436 - mae: 0.8767 - val_loss: 13.4892 - val_mae: 2.4459\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3162 - mae: 0.8519 - val_loss: 14.8362 - val_mae: 2.5348\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2046 - mae: 0.7762 - val_loss: 13.0968 - val_mae: 2.4243\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.4605 - mae: 0.8466 - val_loss: 13.5489 - val_mae: 2.4566\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2556 - mae: 0.8286 - val_loss: 13.3548 - val_mae: 2.4371\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2678 - mae: 0.7513 - val_loss: 13.3106 - val_mae: 2.4544\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.2436 - mae: 0.7918 - val_loss: 14.0457 - val_mae: 2.4877\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3021 - mae: 0.7908 - val_loss: 13.2537 - val_mae: 2.4352\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3970 - mae: 0.8101 - val_loss: 14.5189 - val_mae: 2.5201\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3935 - mae: 0.8255 - val_loss: 12.8740 - val_mae: 2.4186\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.1721 - mae: 0.7707 - val_loss: 14.7716 - val_mae: 2.5571\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2333 - mae: 0.7690 - val_loss: 12.8954 - val_mae: 2.4290\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0414 - mae: 0.7410 - val_loss: 14.2816 - val_mae: 2.5176\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2162 - mae: 0.7618 - val_loss: 14.9177 - val_mae: 2.5228\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0896 - mae: 0.7536 - val_loss: 14.3400 - val_mae: 2.5303\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0705 - mae: 0.7554 - val_loss: 13.1337 - val_mae: 2.4317\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2502 - mae: 0.8221 - val_loss: 14.6798 - val_mae: 2.5933\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.5528 - mae: 0.9288 - val_loss: 14.2200 - val_mae: 2.5234\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3533 - mae: 0.8398 - val_loss: 15.3238 - val_mae: 2.6007\n",
            "Epoch 176/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7170 - mae: 0.9367 - val_loss: 14.4519 - val_mae: 2.5065\n",
            "Epoch 177/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 2.8493 - mae: 1.2058 - val_loss: 18.3437 - val_mae: 2.8965\n",
            "Epoch 178/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.6641 - mae: 1.2818 - val_loss: 13.9308 - val_mae: 2.5559\n",
            "Epoch 179/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3571 - mae: 0.8604 - val_loss: 11.7149 - val_mae: 2.3952\n",
            "Epoch 180/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.6211 - mae: 0.8851 - val_loss: 12.5623 - val_mae: 2.4122\n",
            "Epoch 181/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2661 - mae: 0.8068 - val_loss: 12.1533 - val_mae: 2.4540\n",
            "Epoch 182/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.1466 - mae: 0.7312 - val_loss: 11.8848 - val_mae: 2.4089\n",
            "Epoch 183/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.6335 - mae: 0.9028 - val_loss: 14.7202 - val_mae: 2.6789\n",
            "Epoch 184/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2890 - mae: 0.8469 - val_loss: 14.4832 - val_mae: 2.5994\n",
            "Epoch 185/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2010 - mae: 0.8354 - val_loss: 12.4064 - val_mae: 2.4293\n",
            "Epoch 186/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.1470 - mae: 0.7361 - val_loss: 12.9832 - val_mae: 2.4860\n",
            "Epoch 187/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0053 - mae: 0.7548 - val_loss: 12.1702 - val_mae: 2.4416\n",
            "Epoch 188/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.4240 - mae: 0.8902 - val_loss: 12.2240 - val_mae: 2.3846\n",
            "Epoch 189/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9698 - mae: 0.6633 - val_loss: 12.7917 - val_mae: 2.4228\n",
            "Epoch 190/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3410 - mae: 0.8102 - val_loss: 12.8618 - val_mae: 2.4020\n",
            "Epoch 191/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5709 - mae: 0.9071 - val_loss: 14.6931 - val_mae: 2.6051\n",
            "Epoch 192/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5069 - mae: 0.9417 - val_loss: 13.0790 - val_mae: 2.5076\n",
            "Epoch 193/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5656 - mae: 0.8914 - val_loss: 12.0860 - val_mae: 2.3724\n",
            "Epoch 194/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9434 - mae: 0.6892 - val_loss: 13.7425 - val_mae: 2.5045\n",
            "Epoch 195/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.1359 - mae: 0.7075 - val_loss: 12.2856 - val_mae: 2.3721\n",
            "Epoch 196/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.0136 - mae: 0.6715 - val_loss: 12.7950 - val_mae: 2.4169\n",
            "Epoch 197/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 1.0286 - mae: 0.6741 - val_loss: 13.2975 - val_mae: 2.4233\n",
            "Epoch 198/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.8696 - mae: 0.6501 - val_loss: 13.7201 - val_mae: 2.4747\n",
            "Epoch 199/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7126 - mae: 0.6122 - val_loss: 13.0882 - val_mae: 2.4531\n",
            "Epoch 200/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9634 - mae: 0.6753 - val_loss: 13.1830 - val_mae: 2.4514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6e6be285d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0477HKJbAZO8",
        "outputId": "1d6207f6-3a19-4099-e2a9-45c44aaaeca7"
      },
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ],
      "id": "0477HKJbAZO8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 14.6281 - mae: 2.7003\n",
            "Test Loss: [14.628083229064941, 2.7002830505371094]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HL49K3z0VCl"
      },
      "source": [
        "**REGULARIZACIÓN TIPO 2. REDUCCIÓN A UN MODELO MÁS SIMPLE**"
      ],
      "id": "4HL49K3z0VCl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOYH6y00SzAC"
      },
      "source": [
        "# Fijamos una semilla para asegurar que el resultado es reproducible\n",
        "\n",
        "tf.random.set_seed(1)\n"
      ],
      "id": "nOYH6y00SzAC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hired-ground"
      },
      "source": [
        "# definimos la entrada\n",
        "inputs = keras.Input(shape=(13,), name='input_layer')\n",
        "\n",
        "# Utilizamos la reducción a un modelo más simple como técnica de regularización. Para ello nos quedamos con solo dos capas\n",
        "\n",
        "layer_1 = layers.Dense (64, activation='relu', name = 'layer_1') (inputs)\n",
        "\n",
        "layer_2 = layers.Dense (64, activation='relu', name = 'layer_2') (layer_1)\n",
        "\n",
        "# al final definimos la capa de salida                        \n",
        "outputs = layers.Dense (1, name = 'output_layer') (layer_1) \n",
        "\n",
        "# y creamos el modelo \n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='my_simpler_model')"
      ],
      "id": "hired-ground",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focal-traffic"
      },
      "source": [
        "# Compilación del modelo\n",
        "model.compile(optimizer='adam', \n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])"
      ],
      "id": "focal-traffic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "338f8622"
      },
      "source": [
        "#batch_size=16"
      ],
      "id": "338f8622",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prostate-instrumentation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2cdfe1-6bf6-4b0c-e45e-3b0a5dbba394"
      },
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ],
      "id": "prostate-instrumentation",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 10ms/step - loss: 550.1998 - mae: 21.6895 - val_loss: 608.6171 - val_mae: 22.9587\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 531.2937 - mae: 21.3235 - val_loss: 577.7939 - val_mae: 22.2890\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 482.6608 - mae: 19.9550 - val_loss: 542.3962 - val_mae: 21.5047\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 492.6026 - mae: 20.1568 - val_loss: 499.1669 - val_mae: 20.5243\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 416.3467 - mae: 18.6559 - val_loss: 450.1573 - val_mae: 19.3493\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 377.3060 - mae: 17.3141 - val_loss: 395.2890 - val_mae: 17.9299\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 316.8962 - mae: 15.9372 - val_loss: 340.6300 - val_mae: 16.3957\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 267.9546 - mae: 14.5050 - val_loss: 285.4507 - val_mae: 14.6762\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 200.9870 - mae: 12.4005 - val_loss: 237.8437 - val_mae: 12.9450\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 170.3394 - mae: 10.8590 - val_loss: 195.8531 - val_mae: 11.2334\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 164.5274 - mae: 10.1705 - val_loss: 162.2596 - val_mae: 9.8139\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 132.9408 - mae: 8.8132 - val_loss: 137.3829 - val_mae: 8.8795\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 99.3554 - mae: 7.6490 - val_loss: 118.7038 - val_mae: 8.1204\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 73.8320 - mae: 6.7515 - val_loss: 102.5189 - val_mae: 7.4357\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 76.8197 - mae: 6.4036 - val_loss: 89.7634 - val_mae: 6.8633\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 71.0659 - mae: 6.0438 - val_loss: 79.6104 - val_mae: 6.4150\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 73.8800 - mae: 5.9756 - val_loss: 71.2417 - val_mae: 6.0306\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 37.9413 - mae: 4.5078 - val_loss: 65.1606 - val_mae: 5.7363\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 40.3106 - mae: 4.7255 - val_loss: 59.0975 - val_mae: 5.4659\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 50.4031 - mae: 4.8046 - val_loss: 53.7979 - val_mae: 5.2128\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 32.0001 - mae: 3.7752 - val_loss: 49.7550 - val_mae: 4.9985\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.3462 - mae: 3.9692 - val_loss: 46.5187 - val_mae: 4.8307\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 30.7453 - mae: 3.7806 - val_loss: 43.7301 - val_mae: 4.6845\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.6480 - mae: 3.7586 - val_loss: 41.1067 - val_mae: 4.5720\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 24.0851 - mae: 3.5240 - val_loss: 39.1165 - val_mae: 4.4721\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.7751 - mae: 3.7467 - val_loss: 37.1444 - val_mae: 4.3683\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 33.7545 - mae: 3.7433 - val_loss: 35.7930 - val_mae: 4.2946\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 28.9793 - mae: 3.6458 - val_loss: 34.5566 - val_mae: 4.2419\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 24.7589 - mae: 3.3558 - val_loss: 33.4906 - val_mae: 4.1804\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 22.9738 - mae: 3.3868 - val_loss: 32.5556 - val_mae: 4.1409\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 29.2387 - mae: 3.5136 - val_loss: 31.6795 - val_mae: 4.0968\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.4397 - mae: 3.7223 - val_loss: 30.8643 - val_mae: 4.0599\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 21.6504 - mae: 3.3081 - val_loss: 30.2284 - val_mae: 4.0098\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 20.2349 - mae: 3.3551 - val_loss: 29.2065 - val_mae: 4.0121\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 19.4696 - mae: 3.1814 - val_loss: 28.6561 - val_mae: 3.9867\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 28.9671 - mae: 3.5052 - val_loss: 28.1780 - val_mae: 3.9405\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 17.4058 - mae: 3.1202 - val_loss: 27.6475 - val_mae: 3.9032\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 24.3426 - mae: 3.3175 - val_loss: 26.9379 - val_mae: 3.8575\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 18.7435 - mae: 3.2059 - val_loss: 26.6448 - val_mae: 3.8239\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 22.4594 - mae: 3.2617 - val_loss: 26.1298 - val_mae: 3.8052\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 24.2503 - mae: 3.3646 - val_loss: 25.7062 - val_mae: 3.7707\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 16.8520 - mae: 3.0153 - val_loss: 25.3466 - val_mae: 3.7343\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 20.8826 - mae: 3.2142 - val_loss: 25.0822 - val_mae: 3.7583\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 17.6647 - mae: 3.0481 - val_loss: 24.5874 - val_mae: 3.7038\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 17.5528 - mae: 3.0397 - val_loss: 24.3341 - val_mae: 3.6969\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 17.5698 - mae: 3.0222 - val_loss: 23.9290 - val_mae: 3.6701\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 22.6277 - mae: 3.1906 - val_loss: 23.7440 - val_mae: 3.6655\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 19.1660 - mae: 3.0704 - val_loss: 23.2100 - val_mae: 3.6137\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 19.7094 - mae: 3.0562 - val_loss: 22.9318 - val_mae: 3.6008\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 21.8899 - mae: 3.1295 - val_loss: 22.6184 - val_mae: 3.5649\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 18.6129 - mae: 3.1479 - val_loss: 22.3812 - val_mae: 3.5440\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 19.4421 - mae: 3.1255 - val_loss: 21.9481 - val_mae: 3.5208\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 21.0194 - mae: 3.1613 - val_loss: 21.6891 - val_mae: 3.5051\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 15.2473 - mae: 2.8539 - val_loss: 21.4883 - val_mae: 3.4881\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 14.0020 - mae: 2.8617 - val_loss: 21.2205 - val_mae: 3.4675\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 19.8537 - mae: 3.0157 - val_loss: 20.8720 - val_mae: 3.4498\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 12.1159 - mae: 2.6046 - val_loss: 20.5488 - val_mae: 3.4466\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 14.9843 - mae: 2.7990 - val_loss: 20.4455 - val_mae: 3.4356\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 14.1847 - mae: 2.6685 - val_loss: 20.1461 - val_mae: 3.4087\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 13.7316 - mae: 2.7462 - val_loss: 20.1487 - val_mae: 3.4035\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 17.7628 - mae: 2.8902 - val_loss: 19.8923 - val_mae: 3.3896\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 15.7338 - mae: 2.6694 - val_loss: 19.6418 - val_mae: 3.3714\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 17.5215 - mae: 2.8045 - val_loss: 19.4878 - val_mae: 3.3484\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 12.0641 - mae: 2.6625 - val_loss: 19.1919 - val_mae: 3.3103\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 13.8393 - mae: 2.7045 - val_loss: 19.2019 - val_mae: 3.3111\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 17.8100 - mae: 2.9172 - val_loss: 18.8318 - val_mae: 3.2721\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 13.2899 - mae: 2.6371 - val_loss: 18.7497 - val_mae: 3.2811\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 19.0939 - mae: 2.8728 - val_loss: 18.6748 - val_mae: 3.2574\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 20.4956 - mae: 2.9032 - val_loss: 18.4768 - val_mae: 3.2407\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 15.6779 - mae: 2.6260 - val_loss: 18.2093 - val_mae: 3.2118\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 13.0199 - mae: 2.5145 - val_loss: 18.3703 - val_mae: 3.2480\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.9990 - mae: 2.3689 - val_loss: 17.8051 - val_mae: 3.1917\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 15.2331 - mae: 2.5388 - val_loss: 17.6937 - val_mae: 3.1493\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.5399 - mae: 2.5200 - val_loss: 17.4682 - val_mae: 3.1363\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 11.6422 - mae: 2.3761 - val_loss: 17.3883 - val_mae: 3.1387\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.3707 - mae: 2.5022 - val_loss: 17.2064 - val_mae: 3.0926\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 11.2912 - mae: 2.2858 - val_loss: 17.1026 - val_mae: 3.0908\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 14.9670 - mae: 2.4975 - val_loss: 17.1109 - val_mae: 3.0883\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.2329 - mae: 2.3887 - val_loss: 16.8062 - val_mae: 3.0548\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 11.3884 - mae: 2.4054 - val_loss: 16.6867 - val_mae: 3.0318\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.3507 - mae: 2.4396 - val_loss: 16.5795 - val_mae: 3.0289\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.0408 - mae: 2.2004 - val_loss: 16.3440 - val_mae: 2.9845\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 11.2056 - mae: 2.3536 - val_loss: 16.2795 - val_mae: 2.9896\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.2312 - mae: 2.2062 - val_loss: 16.2124 - val_mae: 2.9729\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 11.9403 - mae: 2.2930 - val_loss: 16.2265 - val_mae: 2.9579\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.4233 - mae: 2.1976 - val_loss: 16.2010 - val_mae: 2.9591\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 10.0394 - mae: 2.1710 - val_loss: 16.0491 - val_mae: 2.9540\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 11.2165 - mae: 2.1899 - val_loss: 16.0320 - val_mae: 2.9250\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 13.5780 - mae: 2.3634 - val_loss: 15.9901 - val_mae: 2.9219\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 12.7867 - mae: 2.3998 - val_loss: 15.8327 - val_mae: 2.8933\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 15.2021 - mae: 2.4041 - val_loss: 15.6905 - val_mae: 2.8932\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 9.8286 - mae: 2.1602 - val_loss: 15.7182 - val_mae: 2.8871\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 12.1192 - mae: 2.3143 - val_loss: 15.5028 - val_mae: 2.8511\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 12.9239 - mae: 2.3839 - val_loss: 15.4996 - val_mae: 2.8312\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 10.5688 - mae: 2.1755 - val_loss: 15.4636 - val_mae: 2.8425\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 7ms/step - loss: 8.5531 - mae: 2.1092 - val_loss: 15.3589 - val_mae: 2.8153\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 8.8059 - mae: 2.1335 - val_loss: 15.3867 - val_mae: 2.8044\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 8.5071 - mae: 2.0708 - val_loss: 15.3658 - val_mae: 2.7934\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 8.7939 - mae: 2.0066 - val_loss: 15.4413 - val_mae: 2.8066\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 8.4963 - mae: 1.9504 - val_loss: 15.4627 - val_mae: 2.8072\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 8.6916 - mae: 2.1200 - val_loss: 15.5195 - val_mae: 2.8057\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 10.0242 - mae: 2.0049 - val_loss: 15.3902 - val_mae: 2.7761\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 10.3158 - mae: 2.1794 - val_loss: 15.3406 - val_mae: 2.7911\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 9.1536 - mae: 2.0074 - val_loss: 15.2479 - val_mae: 2.7604\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 7.5291 - mae: 1.9076 - val_loss: 15.2617 - val_mae: 2.7498\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.4747 - mae: 2.1053 - val_loss: 15.0257 - val_mae: 2.7312\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.7980 - mae: 2.0296 - val_loss: 15.1737 - val_mae: 2.7597\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 8.2520 - mae: 2.0394 - val_loss: 15.1297 - val_mae: 2.7251\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 11.9520 - mae: 2.1553 - val_loss: 15.0449 - val_mae: 2.7299\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.0994 - mae: 1.9767 - val_loss: 14.8828 - val_mae: 2.7184\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 9.9660 - mae: 2.0906 - val_loss: 15.0399 - val_mae: 2.6820\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 12.2300 - mae: 2.1671 - val_loss: 15.0303 - val_mae: 2.7182\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.5914 - mae: 1.9302 - val_loss: 14.7595 - val_mae: 2.6872\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.9114 - mae: 1.9452 - val_loss: 14.9554 - val_mae: 2.7029\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.7149 - mae: 1.9249 - val_loss: 14.8964 - val_mae: 2.6623\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.2166 - mae: 1.9016 - val_loss: 14.8320 - val_mae: 2.6789\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.1029 - mae: 2.0614 - val_loss: 14.9103 - val_mae: 2.6622\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.5549 - mae: 2.1039 - val_loss: 14.8683 - val_mae: 2.7186\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.1718 - mae: 1.9408 - val_loss: 14.6961 - val_mae: 2.6681\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.0967 - mae: 1.9590 - val_loss: 14.7346 - val_mae: 2.6509\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.0465 - mae: 1.9828 - val_loss: 15.4391 - val_mae: 2.6919\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.9739 - mae: 2.0675 - val_loss: 15.2332 - val_mae: 2.7225\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 9.8940 - mae: 2.0936 - val_loss: 14.8034 - val_mae: 2.6728\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 8.4470 - mae: 1.9774 - val_loss: 14.7055 - val_mae: 2.6574\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.5417 - mae: 2.0002 - val_loss: 14.7564 - val_mae: 2.6718\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.3719 - mae: 1.9504 - val_loss: 14.6839 - val_mae: 2.6407\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 9.4917 - mae: 2.0754 - val_loss: 14.6899 - val_mae: 2.6694\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.1504 - mae: 1.9153 - val_loss: 14.5661 - val_mae: 2.6401\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.7345 - mae: 1.9399 - val_loss: 14.6920 - val_mae: 2.6382\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.0812 - mae: 1.9849 - val_loss: 14.5829 - val_mae: 2.6511\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.4843 - mae: 1.9436 - val_loss: 14.6233 - val_mae: 2.6330\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 8.3291 - mae: 1.9355 - val_loss: 14.6245 - val_mae: 2.6460\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.7577 - mae: 1.8558 - val_loss: 14.6718 - val_mae: 2.6636\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.7989 - mae: 2.0673 - val_loss: 14.4776 - val_mae: 2.6536\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.0653 - mae: 1.8779 - val_loss: 14.5408 - val_mae: 2.6182\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.7901 - mae: 2.0483 - val_loss: 14.5505 - val_mae: 2.6449\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.6924 - mae: 2.0031 - val_loss: 14.4507 - val_mae: 2.6238\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 9.3787 - mae: 2.0763 - val_loss: 14.4741 - val_mae: 2.6433\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.2021 - mae: 1.8730 - val_loss: 14.3252 - val_mae: 2.6111\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.9743 - mae: 1.9456 - val_loss: 14.2802 - val_mae: 2.6021\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.5956 - mae: 1.9687 - val_loss: 14.0786 - val_mae: 2.5670\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.9731 - mae: 1.8565 - val_loss: 14.1198 - val_mae: 2.5957\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.0608 - mae: 1.7634 - val_loss: 14.1419 - val_mae: 2.5864\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 10.2572 - mae: 1.9898 - val_loss: 14.1279 - val_mae: 2.5862\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.2483 - mae: 1.9439 - val_loss: 14.2381 - val_mae: 2.5994\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.7833 - mae: 1.9026 - val_loss: 14.2589 - val_mae: 2.5719\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.2989 - mae: 1.8132 - val_loss: 14.2459 - val_mae: 2.5857\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.7564 - mae: 1.8392 - val_loss: 14.2197 - val_mae: 2.5934\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.7690 - mae: 1.9004 - val_loss: 14.0979 - val_mae: 2.5708\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.2106 - mae: 1.8542 - val_loss: 14.1342 - val_mae: 2.5613\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.2661 - mae: 1.9678 - val_loss: 14.1990 - val_mae: 2.5681\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.4254 - mae: 1.7967 - val_loss: 14.7607 - val_mae: 2.5903\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.9101 - mae: 1.9117 - val_loss: 14.3933 - val_mae: 2.6047\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.2468 - mae: 1.7424 - val_loss: 14.3021 - val_mae: 2.5936\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.9571 - mae: 1.8006 - val_loss: 14.1138 - val_mae: 2.5963\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.1337 - mae: 1.8658 - val_loss: 14.5310 - val_mae: 2.5522\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.5785 - mae: 1.8226 - val_loss: 14.3129 - val_mae: 2.6212\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.2037 - mae: 1.7351 - val_loss: 14.0604 - val_mae: 2.5678\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 6.1305 - mae: 1.7864 - val_loss: 14.0445 - val_mae: 2.5804\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.2874 - mae: 1.9600 - val_loss: 14.2107 - val_mae: 2.5820\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 9.8234 - mae: 1.9557 - val_loss: 14.2825 - val_mae: 2.6136\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.6926 - mae: 1.7037 - val_loss: 14.2527 - val_mae: 2.5911\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 6.9461 - mae: 1.8310 - val_loss: 14.1497 - val_mae: 2.5809\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.8122 - mae: 1.8550 - val_loss: 14.2038 - val_mae: 2.5587\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.3185 - mae: 1.9334 - val_loss: 14.1042 - val_mae: 2.5590\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.3384 - mae: 1.8972 - val_loss: 14.1205 - val_mae: 2.5733\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.7246 - mae: 1.9402 - val_loss: 13.8920 - val_mae: 2.5356\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.7058 - mae: 1.9379 - val_loss: 14.0488 - val_mae: 2.5424\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.1699 - mae: 1.6399 - val_loss: 13.8054 - val_mae: 2.5649\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.9711 - mae: 1.9748 - val_loss: 13.8688 - val_mae: 2.5321\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.6754 - mae: 1.7530 - val_loss: 14.0031 - val_mae: 2.5210\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.9092 - mae: 1.8653 - val_loss: 14.1541 - val_mae: 2.5575\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.4339 - mae: 1.7100 - val_loss: 13.9260 - val_mae: 2.5505\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 9.4875 - mae: 1.9429 - val_loss: 14.0428 - val_mae: 2.5656\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 7.2427 - mae: 1.8874 - val_loss: 13.8100 - val_mae: 2.5184\n",
            "Epoch 176/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.6218 - mae: 1.6958 - val_loss: 13.8145 - val_mae: 2.5267\n",
            "Epoch 177/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.4664 - mae: 1.8961 - val_loss: 13.5395 - val_mae: 2.5225\n",
            "Epoch 178/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.7291 - mae: 1.7281 - val_loss: 13.5228 - val_mae: 2.5185\n",
            "Epoch 179/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.8023 - mae: 1.7251 - val_loss: 13.9243 - val_mae: 2.5292\n",
            "Epoch 180/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 6.8444 - mae: 1.8855 - val_loss: 13.8824 - val_mae: 2.5477\n",
            "Epoch 181/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.2082 - mae: 1.8337 - val_loss: 13.7872 - val_mae: 2.5374\n",
            "Epoch 182/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.6504 - mae: 1.8232 - val_loss: 13.8011 - val_mae: 2.5270\n",
            "Epoch 183/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.5604 - mae: 1.8529 - val_loss: 13.5907 - val_mae: 2.4777\n",
            "Epoch 184/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.2962 - mae: 1.7557 - val_loss: 13.8223 - val_mae: 2.5035\n",
            "Epoch 185/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.7412 - mae: 1.7509 - val_loss: 13.8142 - val_mae: 2.5025\n",
            "Epoch 186/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.7158 - mae: 1.6817 - val_loss: 13.8216 - val_mae: 2.5113\n",
            "Epoch 187/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.4238 - mae: 1.8048 - val_loss: 13.8777 - val_mae: 2.5000\n",
            "Epoch 188/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.6525 - mae: 1.7317 - val_loss: 13.9576 - val_mae: 2.5224\n",
            "Epoch 189/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.1723 - mae: 1.7584 - val_loss: 13.8298 - val_mae: 2.5076\n",
            "Epoch 190/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.3355 - mae: 1.8340 - val_loss: 13.7370 - val_mae: 2.4974\n",
            "Epoch 191/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.1621 - mae: 1.7876 - val_loss: 13.8297 - val_mae: 2.5125\n",
            "Epoch 192/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.8425 - mae: 1.6972 - val_loss: 13.7356 - val_mae: 2.4834\n",
            "Epoch 193/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.4680 - mae: 1.7677 - val_loss: 13.6888 - val_mae: 2.5036\n",
            "Epoch 194/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.8022 - mae: 1.8388 - val_loss: 13.6519 - val_mae: 2.4992\n",
            "Epoch 195/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 9.4300 - mae: 1.8984 - val_loss: 13.6689 - val_mae: 2.4985\n",
            "Epoch 196/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 9.7358 - mae: 1.9302 - val_loss: 13.5862 - val_mae: 2.4828\n",
            "Epoch 197/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.4719 - mae: 1.7244 - val_loss: 13.6627 - val_mae: 2.5113\n",
            "Epoch 198/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 7.2920 - mae: 1.8412 - val_loss: 13.5692 - val_mae: 2.4796\n",
            "Epoch 199/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.0854 - mae: 1.6796 - val_loss: 13.4519 - val_mae: 2.4961\n",
            "Epoch 200/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 6.4298 - mae: 1.8163 - val_loss: 13.5067 - val_mae: 2.4721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6e6a54c950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "friendly-powell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f91a7b-d948-426e-971c-4180f3f67062"
      },
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ],
      "id": "friendly-powell",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 18.6685 - mae: 2.8164\n",
            "Test Loss: [18.66849708557129, 2.816351890563965]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUNV74Gg0lOZ"
      },
      "source": [
        "**REGULARIZACIÓN TIPO 3. DROPOUT** "
      ],
      "id": "gUNV74Gg0lOZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BDdVxGDTCDX"
      },
      "source": [
        "# Fijamos una semilla para asegurar que el resultado es reproducible\n",
        "\n",
        "tf.random.set_seed(1)\n"
      ],
      "id": "3BDdVxGDTCDX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkIDvuTZ0rRQ"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# definimos la entrada\n",
        "inputs = keras.Input(shape=(13,), name='input_layer')\n",
        "\n",
        "# Añadimos el dropout en cada  capa\n",
        "\n",
        "# capa oculta 1\n",
        "layer_1 = layers.Dense (64, activation='relu', name = 'layer_1') (inputs)\n",
        "layer_1 = layers.Dropout (0.5, name= 'dropout_layer1')(layer_1)\n",
        "\n",
        "# capa oculta 2\n",
        "layer_2 = layers.Dense (64, activation='relu', name = 'layer_2') (layer_1)\n",
        "layer_2 = layers.Dropout (0.5, name= 'dropout_layer2')(layer_2)\n",
        "\n",
        "# capa oculta 3\n",
        "layer_3 = layers.Dense (64, activation='relu', name = 'layer_3') (layer_2)\n",
        "layer_3 = layers.Dropout (0.5, name= 'dropout_layer3')(layer_3)\n",
        "\n",
        "# capa oculta 4\n",
        "layer_4 = layers.Dense (64, activation='relu', name = 'layer_4') (layer_3)\n",
        "layer_4 = layers.Dropout (0.5, name= 'dropout_layer4')(layer_4)\n",
        "\n",
        "\n",
        "# al final definimos la capa de salida                        \n",
        "outputs = layers.Dense (1, name = 'output_layer') (layer_4) \n",
        "\n",
        "# y creamos el modelo \n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='my_model')"
      ],
      "id": "nkIDvuTZ0rRQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isr9iNhJ0yb4"
      },
      "source": [
        "# Compilación del modelo\n",
        "model.compile(optimizer='adam', \n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])"
      ],
      "id": "Isr9iNhJ0yb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4q-pgWm1Fzg",
        "outputId": "3d9c446a-6c32-4e35-fc71-d8074645400f"
      },
      "source": [
        "# No modifique el código\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ],
      "id": "_4q-pgWm1Fzg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 12ms/step - loss: 551.6513 - mae: 21.6653 - val_loss: 594.7106 - val_mae: 22.5204\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 476.3293 - mae: 19.8707 - val_loss: 427.8578 - val_mae: 18.4932\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 271.6260 - mae: 13.6000 - val_loss: 129.4023 - val_mae: 8.4934\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 139.4461 - mae: 9.1579 - val_loss: 91.0836 - val_mae: 7.0030\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 133.4792 - mae: 8.7761 - val_loss: 115.6657 - val_mae: 8.4266\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 131.8579 - mae: 8.4037 - val_loss: 73.3221 - val_mae: 6.3369\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 106.5650 - mae: 7.6672 - val_loss: 82.4343 - val_mae: 7.0102\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 109.1734 - mae: 8.2124 - val_loss: 71.1448 - val_mae: 6.4247\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 92.7099 - mae: 7.5732 - val_loss: 60.4684 - val_mae: 5.8663\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 93.2240 - mae: 7.4091 - val_loss: 62.7730 - val_mae: 6.1870\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 98.6498 - mae: 7.3890 - val_loss: 71.4954 - val_mae: 6.7735\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 103.7656 - mae: 7.5402 - val_loss: 54.7900 - val_mae: 5.7729\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 80.1136 - mae: 6.9787 - val_loss: 54.1682 - val_mae: 5.7207\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 89.2446 - mae: 7.0581 - val_loss: 62.7709 - val_mae: 6.1990\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 87.7036 - mae: 7.1704 - val_loss: 59.9881 - val_mae: 6.0189\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 84.0844 - mae: 7.1552 - val_loss: 63.5713 - val_mae: 6.3543\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 84.8632 - mae: 6.7142 - val_loss: 48.5666 - val_mae: 5.3252\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 67.5832 - mae: 6.4497 - val_loss: 40.5207 - val_mae: 4.6847\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 62.5509 - mae: 6.0844 - val_loss: 53.9162 - val_mae: 5.8811\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 85.6955 - mae: 6.9841 - val_loss: 53.8735 - val_mae: 5.9167\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 70.9586 - mae: 6.4655 - val_loss: 49.8805 - val_mae: 5.5842\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 82.4340 - mae: 6.9129 - val_loss: 45.2015 - val_mae: 5.1310\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 73.2796 - mae: 6.4228 - val_loss: 61.8095 - val_mae: 6.4105\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 72.6428 - mae: 6.4106 - val_loss: 38.0752 - val_mae: 4.6455\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 65.7740 - mae: 5.8574 - val_loss: 55.4226 - val_mae: 6.0339\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 79.3877 - mae: 6.6822 - val_loss: 30.4152 - val_mae: 4.0123\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 74.8784 - mae: 6.5311 - val_loss: 58.0076 - val_mae: 6.3276\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 66.7101 - mae: 6.2666 - val_loss: 29.9992 - val_mae: 4.1174\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 63.9070 - mae: 5.9856 - val_loss: 47.4966 - val_mae: 5.5531\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 55.8863 - mae: 5.6127 - val_loss: 48.8141 - val_mae: 5.6941\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 70.6313 - mae: 6.3909 - val_loss: 42.4193 - val_mae: 5.1761\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 65.7928 - mae: 5.9015 - val_loss: 48.0567 - val_mae: 5.6085\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 46.9011 - mae: 5.0737 - val_loss: 40.5520 - val_mae: 4.9924\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 55.2331 - mae: 5.7502 - val_loss: 45.7210 - val_mae: 5.4051\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 69.8952 - mae: 6.0354 - val_loss: 32.1889 - val_mae: 4.1959\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 68.5853 - mae: 5.9912 - val_loss: 29.8818 - val_mae: 4.0932\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 50.8515 - mae: 5.3461 - val_loss: 44.2195 - val_mae: 5.3528\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 58.1756 - mae: 5.6956 - val_loss: 58.3413 - val_mae: 6.3138\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 72.0381 - mae: 6.2722 - val_loss: 30.0854 - val_mae: 4.1140\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 59.7519 - mae: 5.7591 - val_loss: 28.7107 - val_mae: 4.0616\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 63.9347 - mae: 6.0606 - val_loss: 35.5633 - val_mae: 4.7320\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 55.5848 - mae: 5.4173 - val_loss: 39.2814 - val_mae: 5.0286\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 55.2081 - mae: 5.5188 - val_loss: 40.3804 - val_mae: 5.0022\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 64.1516 - mae: 6.0129 - val_loss: 35.3055 - val_mae: 4.5637\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.5061 - mae: 4.9252 - val_loss: 32.0111 - val_mae: 4.2522\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 54.9969 - mae: 5.3775 - val_loss: 42.5699 - val_mae: 5.2496\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 59.0832 - mae: 5.3876 - val_loss: 26.6067 - val_mae: 3.7483\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 54.6335 - mae: 5.5200 - val_loss: 50.6364 - val_mae: 5.7309\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 50.4635 - mae: 5.3799 - val_loss: 24.4679 - val_mae: 3.5708\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 58.0343 - mae: 5.6628 - val_loss: 31.3822 - val_mae: 4.3661\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 57.5816 - mae: 5.5919 - val_loss: 28.7715 - val_mae: 4.1421\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 46.8492 - mae: 5.1921 - val_loss: 33.3120 - val_mae: 4.5229\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 66.2304 - mae: 5.7042 - val_loss: 35.5539 - val_mae: 4.7316\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 43.1849 - mae: 4.9070 - val_loss: 47.4308 - val_mae: 5.5731\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 41.1586 - mae: 4.8769 - val_loss: 29.8683 - val_mae: 4.1516\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 61.0414 - mae: 5.8925 - val_loss: 38.1344 - val_mae: 4.8885\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 50.7374 - mae: 5.0227 - val_loss: 30.4115 - val_mae: 4.2151\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 53.9009 - mae: 5.2211 - val_loss: 30.9718 - val_mae: 4.2885\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 58.5552 - mae: 5.3371 - val_loss: 42.3715 - val_mae: 5.1128\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 49.3713 - mae: 5.2088 - val_loss: 31.3157 - val_mae: 4.1473\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 41.4762 - mae: 4.8804 - val_loss: 33.6577 - val_mae: 4.3307\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 57.1777 - mae: 5.6525 - val_loss: 40.0011 - val_mae: 5.0077\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 44.5789 - mae: 4.8437 - val_loss: 29.5668 - val_mae: 4.1424\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 35.7536 - mae: 4.3340 - val_loss: 29.1722 - val_mae: 4.1161\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 46.9954 - mae: 5.3011 - val_loss: 48.2820 - val_mae: 5.6090\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 49.0866 - mae: 5.0966 - val_loss: 47.7623 - val_mae: 5.4185\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 44.1202 - mae: 5.1288 - val_loss: 33.5262 - val_mae: 4.2681\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 50.8806 - mae: 5.1037 - val_loss: 39.3505 - val_mae: 4.7162\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 53.7007 - mae: 5.2807 - val_loss: 26.2704 - val_mae: 3.6446\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 47.9390 - mae: 5.2514 - val_loss: 38.8753 - val_mae: 4.7905\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 51.5223 - mae: 5.4768 - val_loss: 41.4595 - val_mae: 5.0054\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 45.9163 - mae: 4.9808 - val_loss: 27.6664 - val_mae: 3.8833\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 66.4200 - mae: 5.5927 - val_loss: 44.9091 - val_mae: 5.2001\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 47.4691 - mae: 5.2185 - val_loss: 25.7423 - val_mae: 3.6197\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 57.0861 - mae: 5.3476 - val_loss: 37.1877 - val_mae: 4.6359\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 43.7361 - mae: 5.1548 - val_loss: 30.9124 - val_mae: 4.1615\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 44.6719 - mae: 5.1641 - val_loss: 39.5797 - val_mae: 4.7577\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 50.3726 - mae: 5.1251 - val_loss: 33.9585 - val_mae: 4.3531\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 45.1684 - mae: 4.7841 - val_loss: 38.9559 - val_mae: 4.7099\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 44.2105 - mae: 5.1149 - val_loss: 45.3675 - val_mae: 5.1392\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 39.0971 - mae: 4.4206 - val_loss: 31.6388 - val_mae: 4.1187\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 36.6312 - mae: 4.1971 - val_loss: 34.1446 - val_mae: 4.3807\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 46.8965 - mae: 4.8585 - val_loss: 36.2521 - val_mae: 4.5706\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.6458 - mae: 5.0306 - val_loss: 26.1399 - val_mae: 3.8262\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 47.3782 - mae: 4.9077 - val_loss: 48.5849 - val_mae: 5.5235\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.7981 - mae: 5.1412 - val_loss: 24.0448 - val_mae: 3.5726\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 52.0807 - mae: 5.3386 - val_loss: 47.8074 - val_mae: 5.3960\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 50.3849 - mae: 5.2456 - val_loss: 34.1781 - val_mae: 4.3555\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 47.1554 - mae: 4.9885 - val_loss: 33.3516 - val_mae: 4.2712\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 47.0218 - mae: 5.0395 - val_loss: 31.2722 - val_mae: 4.1149\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 57.9235 - mae: 5.2159 - val_loss: 30.8748 - val_mae: 4.1577\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 40.7459 - mae: 4.8280 - val_loss: 37.9757 - val_mae: 4.7481\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 49.1219 - mae: 5.1197 - val_loss: 28.3724 - val_mae: 3.9451\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 56.2370 - mae: 5.2351 - val_loss: 54.0621 - val_mae: 5.7587\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 41.9177 - mae: 4.7322 - val_loss: 31.9599 - val_mae: 4.1529\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 48.5613 - mae: 5.0253 - val_loss: 35.0988 - val_mae: 4.3594\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 41.8557 - mae: 4.8203 - val_loss: 32.1204 - val_mae: 4.1078\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 46.9430 - mae: 5.1915 - val_loss: 34.8704 - val_mae: 4.4011\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 44.2327 - mae: 4.6913 - val_loss: 28.0066 - val_mae: 3.8946\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 40.4199 - mae: 4.7711 - val_loss: 21.6138 - val_mae: 3.3209\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 37.5846 - mae: 4.5440 - val_loss: 29.1825 - val_mae: 3.9752\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.7557 - mae: 4.8489 - val_loss: 32.0436 - val_mae: 4.2081\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 38.5198 - mae: 4.8606 - val_loss: 27.9222 - val_mae: 3.8687\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 44.2539 - mae: 4.8701 - val_loss: 46.7695 - val_mae: 5.2303\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 39.2871 - mae: 4.8210 - val_loss: 23.8222 - val_mae: 3.4625\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.0462 - mae: 4.8560 - val_loss: 31.4848 - val_mae: 4.0660\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 40.5827 - mae: 4.6963 - val_loss: 40.1334 - val_mae: 4.6176\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.1773 - mae: 4.3885 - val_loss: 31.6391 - val_mae: 4.0150\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 57.1916 - mae: 5.2368 - val_loss: 35.9834 - val_mae: 4.3095\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.7960 - mae: 4.3955 - val_loss: 37.2873 - val_mae: 4.4716\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 43.2164 - mae: 4.9052 - val_loss: 27.9952 - val_mae: 3.7502\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 50.6306 - mae: 4.9841 - val_loss: 30.2464 - val_mae: 4.0581\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.9715 - mae: 4.8040 - val_loss: 25.9750 - val_mae: 3.6343\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 48.1662 - mae: 5.2006 - val_loss: 27.9355 - val_mae: 3.8222\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 40.7352 - mae: 4.8746 - val_loss: 36.3979 - val_mae: 4.4640\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 37.4971 - mae: 4.7462 - val_loss: 26.7615 - val_mae: 3.6781\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 37.5346 - mae: 4.4952 - val_loss: 27.8028 - val_mae: 3.7918\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 37.3950 - mae: 4.3108 - val_loss: 28.9759 - val_mae: 3.9331\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 39.5359 - mae: 4.5209 - val_loss: 40.8388 - val_mae: 4.8422\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 44.9070 - mae: 5.1330 - val_loss: 31.0957 - val_mae: 4.0530\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 31.9907 - mae: 4.1594 - val_loss: 24.1470 - val_mae: 3.5599\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 41.1867 - mae: 4.7008 - val_loss: 24.7610 - val_mae: 3.5870\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 45.8469 - mae: 4.9354 - val_loss: 38.6416 - val_mae: 4.6515\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 51.2960 - mae: 5.1164 - val_loss: 24.2504 - val_mae: 3.5609\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 41.0867 - mae: 4.7332 - val_loss: 27.8286 - val_mae: 3.8827\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.5670 - mae: 4.9327 - val_loss: 21.3327 - val_mae: 3.3617\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 51.2545 - mae: 5.2079 - val_loss: 26.5445 - val_mae: 3.7534\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 41.9843 - mae: 4.8945 - val_loss: 29.8359 - val_mae: 4.0164\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 40.3454 - mae: 4.6784 - val_loss: 30.3679 - val_mae: 4.0364\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 51.1828 - mae: 5.0957 - val_loss: 34.2230 - val_mae: 4.3471\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 30.9315 - mae: 4.2813 - val_loss: 22.7653 - val_mae: 3.4572\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 39.2750 - mae: 4.4969 - val_loss: 30.1113 - val_mae: 4.0416\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 35.5262 - mae: 4.3509 - val_loss: 27.9210 - val_mae: 3.8459\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 33.1481 - mae: 4.3903 - val_loss: 31.6667 - val_mae: 4.1787\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 36.7041 - mae: 4.4531 - val_loss: 31.0629 - val_mae: 4.1319\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 33.2850 - mae: 4.1969 - val_loss: 25.1666 - val_mae: 3.6275\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 38.3187 - mae: 4.6421 - val_loss: 31.2944 - val_mae: 4.0887\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 45.4273 - mae: 4.9331 - val_loss: 27.2933 - val_mae: 3.7768\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 36.6474 - mae: 4.4908 - val_loss: 22.3468 - val_mae: 3.4442\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 42.0846 - mae: 4.6588 - val_loss: 30.9760 - val_mae: 4.1951\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 35.8068 - mae: 4.4128 - val_loss: 21.5492 - val_mae: 3.4613\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 30.9478 - mae: 4.1851 - val_loss: 24.2661 - val_mae: 3.6728\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 35.4262 - mae: 4.6994 - val_loss: 19.0792 - val_mae: 3.2614\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 33.5863 - mae: 4.2432 - val_loss: 22.3845 - val_mae: 3.6270\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 41.4432 - mae: 4.6716 - val_loss: 29.8367 - val_mae: 4.1961\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 45.0976 - mae: 4.9826 - val_loss: 23.7805 - val_mae: 3.5663\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.4872 - mae: 4.2438 - val_loss: 18.8756 - val_mae: 3.1194\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 47.7098 - mae: 5.0829 - val_loss: 36.9184 - val_mae: 4.4883\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.9967 - mae: 4.3396 - val_loss: 18.5441 - val_mae: 3.0134\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 34.3206 - mae: 4.3622 - val_loss: 34.5852 - val_mae: 4.3075\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 41.0180 - mae: 4.6187 - val_loss: 34.9358 - val_mae: 4.3402\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 37.3108 - mae: 4.4847 - val_loss: 26.1058 - val_mae: 3.6895\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 35.9490 - mae: 4.4483 - val_loss: 28.6956 - val_mae: 3.8684\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.7365 - mae: 4.2146 - val_loss: 24.4894 - val_mae: 3.5773\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 29.5065 - mae: 3.8969 - val_loss: 29.4840 - val_mae: 3.9271\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 39.3155 - mae: 4.4258 - val_loss: 36.3870 - val_mae: 4.3804\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 30.4953 - mae: 4.1290 - val_loss: 25.4729 - val_mae: 3.5586\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 41.3550 - mae: 4.6317 - val_loss: 25.6747 - val_mae: 3.5794\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.9465 - mae: 4.3017 - val_loss: 27.1214 - val_mae: 3.7053\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 42.9734 - mae: 4.9707 - val_loss: 27.7445 - val_mae: 3.7556\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 31.6914 - mae: 4.0846 - val_loss: 28.3893 - val_mae: 3.7778\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.9604 - mae: 4.4293 - val_loss: 26.9162 - val_mae: 3.6736\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 38.6894 - mae: 4.5567 - val_loss: 20.7983 - val_mae: 3.2411\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 38.4500 - mae: 4.5543 - val_loss: 22.1885 - val_mae: 3.3669\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 33.8196 - mae: 4.1901 - val_loss: 28.5738 - val_mae: 3.8842\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 39.1033 - mae: 4.4045 - val_loss: 27.1037 - val_mae: 3.7341\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 28.4219 - mae: 4.0640 - val_loss: 22.7813 - val_mae: 3.3377\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 43.9878 - mae: 4.7230 - val_loss: 23.4102 - val_mae: 3.4157\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 44.7497 - mae: 4.8729 - val_loss: 30.8986 - val_mae: 4.0007\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 41.4745 - mae: 4.8103 - val_loss: 29.9735 - val_mae: 3.8865\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 32.2149 - mae: 4.2072 - val_loss: 27.0394 - val_mae: 3.6584\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 35.2956 - mae: 4.2144 - val_loss: 22.1060 - val_mae: 3.2777\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 39.4513 - mae: 4.5303 - val_loss: 26.8596 - val_mae: 3.6638\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 42.8048 - mae: 4.7267 - val_loss: 26.6207 - val_mae: 3.6301\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.4019 - mae: 4.1037 - val_loss: 20.2554 - val_mae: 3.1272\n",
            "Epoch 176/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 37.9269 - mae: 4.5272 - val_loss: 28.7183 - val_mae: 3.8354\n",
            "Epoch 177/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.5073 - mae: 4.4372 - val_loss: 22.1450 - val_mae: 3.3030\n",
            "Epoch 178/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 35.8124 - mae: 4.2610 - val_loss: 35.3668 - val_mae: 4.2905\n",
            "Epoch 179/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.0162 - mae: 4.4145 - val_loss: 35.6513 - val_mae: 4.2382\n",
            "Epoch 180/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 37.4048 - mae: 4.4107 - val_loss: 22.6526 - val_mae: 3.2859\n",
            "Epoch 181/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 39.4021 - mae: 4.5686 - val_loss: 27.2034 - val_mae: 3.6582\n",
            "Epoch 182/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 38.0960 - mae: 4.2965 - val_loss: 27.7220 - val_mae: 3.7287\n",
            "Epoch 183/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 33.8265 - mae: 4.2373 - val_loss: 33.7505 - val_mae: 4.1238\n",
            "Epoch 184/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 29.7196 - mae: 4.1754 - val_loss: 28.1492 - val_mae: 3.7390\n",
            "Epoch 185/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 31.1720 - mae: 4.2062 - val_loss: 31.0752 - val_mae: 3.8901\n",
            "Epoch 186/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 38.6154 - mae: 4.5354 - val_loss: 27.7685 - val_mae: 3.6246\n",
            "Epoch 187/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 29.3681 - mae: 4.2301 - val_loss: 26.1728 - val_mae: 3.5277\n",
            "Epoch 188/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.7149 - mae: 4.3300 - val_loss: 36.5227 - val_mae: 4.2711\n",
            "Epoch 189/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.0910 - mae: 4.4094 - val_loss: 19.8552 - val_mae: 3.0397\n",
            "Epoch 190/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 33.7181 - mae: 4.1019 - val_loss: 20.2073 - val_mae: 3.1536\n",
            "Epoch 191/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 51.0432 - mae: 4.7402 - val_loss: 22.0095 - val_mae: 3.3517\n",
            "Epoch 192/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 35.7669 - mae: 4.6142 - val_loss: 26.6514 - val_mae: 3.7140\n",
            "Epoch 193/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 31.6997 - mae: 4.1761 - val_loss: 31.2443 - val_mae: 3.9417\n",
            "Epoch 194/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.6079 - mae: 4.3121 - val_loss: 27.9061 - val_mae: 3.6669\n",
            "Epoch 195/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 37.4609 - mae: 4.2466 - val_loss: 21.2322 - val_mae: 3.2119\n",
            "Epoch 196/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.0698 - mae: 4.3667 - val_loss: 31.9440 - val_mae: 4.0405\n",
            "Epoch 197/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.9170 - mae: 4.1718 - val_loss: 25.8318 - val_mae: 3.5222\n",
            "Epoch 198/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 34.6318 - mae: 4.3290 - val_loss: 29.2922 - val_mae: 3.8747\n",
            "Epoch 199/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 32.5943 - mae: 4.3039 - val_loss: 26.9581 - val_mae: 3.6968\n",
            "Epoch 200/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 32.0067 - mae: 4.0974 - val_loss: 21.8865 - val_mae: 3.2360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6e6fd860d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJaBVrqA1GcB",
        "outputId": "d9a96e20-e361-4a9b-ff05-c41597c646a3"
      },
      "source": [
        "# No modifique el código\n",
        "results = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ],
      "id": "lJaBVrqA1GcB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 27.4320 - mae: 3.8757\n",
            "Test Loss: [27.432024002075195, 3.8757386207580566]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6umqh11UykO"
      },
      "source": [
        "**Solución optima:** \n",
        "Reduciendo el batch_size de 32 a 16 hamos consegido una reducción del test lost de 18.9684 a 14.6281"
      ],
      "id": "Z6umqh11UykO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "british-vegetation"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "## Cuestión 3: Utilice el mismo modelo de la cuestión anterior pero añadiendo un callback de early stopping. Obtenga un test loss inferior al del modelo anterior"
      ],
      "id": "british-vegetation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di_2_uB3Yvjs"
      },
      "source": [
        "# definir el early stopping callback\n",
        "es_callback = keras.callbacks.EarlyStopping(\n",
        " monitor ='val_loss',\n",
        " patience=15,\n",
        " verbose=1)"
      ],
      "id": "di_2_uB3Yvjs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "precise-finish"
      },
      "source": [
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "inputs = keras.Input(shape=(13,), name='input_layer')\n",
        "\n",
        "layer_1 = layers.Dense (64, activation='relu', name = 'layer_1') (inputs)\n",
        "layer_2 = layers.Dense (64, activation='relu', name = 'layer_2') (layer_1)\n",
        "layer_3 = layers.Dense (64, activation='relu', name = 'layer_3') (layer_2)\n",
        "layer_4 = layers.Dense (64, activation='relu', name = 'layer_4') (layer_3)\n",
        "                       \n",
        "outputs = layers.Dense (1, name = 'output_layer') (layer_4) \n",
        "\n",
        "model_early_stopping = keras.Model(inputs=inputs, outputs=outputs, name='my_model') "
      ],
      "id": "precise-finish",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blond-telephone"
      },
      "source": [
        "# Compilación del modelo\n",
        "model_early_stopping.compile(optimizer='adam', \n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])"
      ],
      "id": "blond-telephone",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "subsequent-roads",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b69e1f0-a344-42fb-cbd7-d04e102822cd"
      },
      "source": [
        "history_early_stopping = model_early_stopping.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          epochs=200,\n",
        "          batch_size=16,\n",
        "          validation_split=0.2,\n",
        "          verbose=1,\n",
        "          callbacks=[es_callback]) "
      ],
      "id": "subsequent-roads",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 12ms/step - loss: 6.2269 - mae: 1.9589 - val_loss: 17.0569 - val_mae: 2.8717\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 8.0053 - mae: 2.1154 - val_loss: 15.5717 - val_mae: 2.9568\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.6877 - mae: 1.7764 - val_loss: 15.7925 - val_mae: 2.7386\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.2121 - mae: 1.8526 - val_loss: 14.1410 - val_mae: 2.6551\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.6054 - mae: 1.5279 - val_loss: 14.3588 - val_mae: 2.5753\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.8015 - mae: 1.7084 - val_loss: 14.6579 - val_mae: 2.6517\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.1998 - mae: 1.6925 - val_loss: 15.9076 - val_mae: 2.6843\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.9945 - mae: 1.7245 - val_loss: 18.3606 - val_mae: 3.0327\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.9467 - mae: 1.7202 - val_loss: 14.4414 - val_mae: 2.6484\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.8530 - mae: 1.7865 - val_loss: 14.3977 - val_mae: 2.6389\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 6.2192 - mae: 1.7898 - val_loss: 16.9184 - val_mae: 2.8759\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.6566 - mae: 1.6612 - val_loss: 14.4347 - val_mae: 2.5870\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.4265 - mae: 1.6692 - val_loss: 16.9677 - val_mae: 2.8316\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.9866 - mae: 1.6131 - val_loss: 14.4693 - val_mae: 2.6887\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.5206 - mae: 1.6984 - val_loss: 15.6756 - val_mae: 2.7150\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.1442 - mae: 1.4195 - val_loss: 15.4883 - val_mae: 2.6530\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.3397 - mae: 1.5591 - val_loss: 15.3559 - val_mae: 2.6738\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.9444 - mae: 1.3978 - val_loss: 15.9104 - val_mae: 2.6680\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.6943 - mae: 1.5866 - val_loss: 16.5192 - val_mae: 2.6411\n",
            "Epoch 00019: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pressing-object",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4a58c2-4fe3-4499-c620-f19c64bd4997"
      },
      "source": [
        "# No modifique el código\n",
        "results = model_early_stopping.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test Loss: {}'.format(results))"
      ],
      "id": "pressing-object",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 16.0201 - mae: 2.7015\n",
            "Test Loss: [16.020097732543945, 2.701547384262085]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addressed-lesbian"
      },
      "source": [
        "<a name='1.4'></a>\n",
        "## Cuestión 4: ¿Podría haberse usado otra función de activación de la neurona de salida? En caso afirmativo especifíquela."
      ],
      "id": "addressed-lesbian"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruled-silicon"
      },
      "source": [
        "La función de activación que hemos utilizado es la linea, que es la que por defecto se aplica cuando no se especifica. Podríamos haber utilizado una relu porque estamos se trata de una regresión positiva, es decir, que los resultados son positivos. "
      ],
      "id": "ruled-silicon"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "robust-christianity"
      },
      "source": [
        "<a name='1.5'></a>\n",
        "## Cuestión 5:  ¿Qué es lo que una neurona calcula?\n",
        "\n",
        "**a)** Una función de activación seguida de una suma ponderada  de las entradas.\n",
        "\n",
        "**b)** Una suma ponderada  de las entradas seguida de una función de activación.\n",
        "\n",
        "**c)** Una función de pérdida, definida sobre el target.\n",
        "\n",
        "**d)** Ninguna  de las anteriores es correcta\n"
      ],
      "id": "robust-christianity"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joined-burden"
      },
      "source": [
        "b) Una suma ponderada de las entradas seguida de una función de activación"
      ],
      "id": "joined-burden"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iraqi-european"
      },
      "source": [
        "<a name='1.6'></a>\n",
        "## Cuestión 6:  ¿Cuál de estas funciones de activación no debería usarse en una capa oculta (hidden layer)?\n",
        "\n",
        "**a)** `sigmoid`\n",
        "\n",
        "**b)** `tanh`\n",
        "\n",
        "**c)** `relu`\n",
        "\n",
        "**d)** `linear`\n"
      ],
      "id": "iraqi-european"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cardiovascular-attack"
      },
      "source": [
        "d) linear"
      ],
      "id": "cardiovascular-attack"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranging-utilization"
      },
      "source": [
        "<a name='1.7'></a>\n",
        "## Cuestión 7:  ¿Cuál de estas técnicas es efectiva para combatir el overfitting en una red con varias capas ocultas? Ponga todas las que lo sean.\n",
        "\n",
        "**a)** Dropout\n",
        "\n",
        "**b)** Regularización L2.\n",
        "\n",
        "**c)** Aumentar el tamaño del test set.\n",
        "\n",
        "**d)** Aumentar el tamaño del validation set.\n",
        "\n",
        "**e)** Reducir el número de capas de la red.\n",
        "\n",
        "**f)** Data augmentation."
      ],
      "id": "ranging-utilization"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accessible-trainer"
      },
      "source": [
        "a) Dropout, b) Regularización L2, e) Reducir el número de capas de la red, f) Data augmentation"
      ],
      "id": "accessible-trainer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recreational-deposit"
      },
      "source": [
        "<a name='1.8'></a>\n",
        "## Cuestión 8:  Supongamos que queremos entrenar una red para un problema de clasificación de imágenes con las siguientes clases: {'perro','gato','persona'}. ¿Cuántas neuronas y que función de activación debería tener la capa de salida? ¿Qué función de pérdida (loss function) debería usarse?\n"
      ],
      "id": "recreational-deposit"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3S3-EhLhjpy"
      },
      "source": [
        ""
      ],
      "id": "I3S3-EhLhjpy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "confirmed-roulette"
      },
      "source": [
        "Un problema de clasificación en el que tenemos varias clases requiere de la la aplicación de una función de activación debe ser la softmax. \n",
        "\n",
        "La función de de pérdida adecuada es la categorical cross-entropy"
      ],
      "id": "confirmed-roulette"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "painful-decade"
      },
      "source": [
        "<a name='actividad_2'></a>\n",
        "# Actividad 2: Redes Convolucionales\n",
        "\n",
        "Vamos a usar el dataset [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html), que son 60000 imágenes de 32x32 a color  con 10 clases diferentes. Para realizar mejor la práctica puede consultar [Introduction_to_CNN.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Introduction_to_CNN.ipynb).\n",
        "\n",
        "\n",
        "\n",
        "**Puntuación**: \n",
        "\n",
        "- [Cuestión 1](#2.1): 1 pt\n",
        "- [Cuestión 2](#2.2): 1.5 pt\n",
        "- [Cuestión 3](#2.3): 0.5 pts\n",
        "- [Cuestión 4](#2.4): 0.5 pts\n",
        "- [Cuestión 5](#2.5): 0.5 pts\n",
        "- [Cuestión 6](#2.6): 0.5 pts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Puede normalizar las imágenes al principio o usar la capa [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling):\n",
        "\n",
        "```python\n",
        "tf.keras.layers.experimental.preprocessing.Rescaling(\n",
        "    scale, offset=0.0, name=None, **kwargs\n",
        ")\n",
        "```"
      ],
      "id": "painful-decade"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incorporate-terrorist"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()"
      ],
      "id": "incorporate-terrorist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brazilian-rhythm"
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i])\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "plt.show()"
      ],
      "id": "brazilian-rhythm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extreme-quantum"
      },
      "source": [
        "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
        "print('x_test, y_test shapes:', x_test.shape, y_test.shape)"
      ],
      "id": "extreme-quantum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "living-philosophy"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "## Cuestión 1: Cree una red convolucional con la API funcional con al menos dos capas convolucionales y al menos dos capas de pooling. Utilize sólo [Average Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) y no añada ninguna regularización."
      ],
      "id": "living-philosophy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atmospheric-sight"
      },
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ],
      "id": "atmospheric-sight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "needed-arena"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "id": "needed-arena",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pursuant-paper"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=25, batch_size=64,\n",
        "                    validation_split=0.15)"
      ],
      "id": "pursuant-paper",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "applicable-honduras"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ],
      "id": "applicable-honduras",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "numerous-invite"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "## Cuestión 2: Cree un modelo con la API funcional con un máximo de 2 capas convolucionales y un máximo de 2 capas de pooling. Utilize  [Max Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) o [Average Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) y  añada la regularización que quiera. Debe obtener un `Test accuracy > 0.68`"
      ],
      "id": "numerous-invite"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "annual-diploma"
      },
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ],
      "id": "annual-diploma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "indian-messaging"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "id": "indian-messaging",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "functional-republic"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=..., batch_size=...,\n",
        "                    validation_split=0.15, callbacks=lbacks=[...])"
      ],
      "id": "functional-republic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "incorrect-completion"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ],
      "id": "incorrect-completion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optical-arizona"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "## Cuestión 3: Añada data augmentation al principio del modelo\n",
        "\n"
      ],
      "id": "optical-arizona"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "previous-boxing"
      },
      "source": [
        "data_augmentation=... "
      ],
      "id": "previous-boxing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comprehensive-directive"
      },
      "source": [
        "inputs = tf.keras.Input(shape=..., name='input')\n",
        "data_aug= ...\n",
        "\n",
        "# reescaling = ...\n",
        "\n",
        "# Convolution + pooling layers\n",
        "...\n",
        "\n",
        "# Flattening\n",
        "...\n",
        "\n",
        "# Fully-connected\n",
        "outputs = layers.Dense(...)\n",
        "\n",
        "model = keras.Model(inputs=..., outputs=...)"
      ],
      "id": "comprehensive-directive",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "statutory-covering"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "id": "statutory-covering",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "western-energy"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=..., batch_size=...,\n",
        "                    validation_split=0.15, callbacks=lbacks=[...])"
      ],
      "id": "western-energy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "classical-charm"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=0, batch_size=1000)\n",
        "print('Test Loss: {}'.format(results[0]))\n",
        "print('Test Accuracy: {}'.format(results[1]))"
      ],
      "id": "classical-charm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sweet-implement"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "## Cuestión 4: Cree el mismo  modelo de manera secuencial. No es necesario compilar ni entrenar el modelo"
      ],
      "id": "sweet-implement"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auburn-lawrence"
      },
      "source": [
        "model_seq = tf.keras.models.Sequential()\n",
        "# Código aquí\n",
        "..."
      ],
      "id": "auburn-lawrence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "present-consortium"
      },
      "source": [
        "<a name='2.5'></a>\n",
        "## Cuestión 5: Si tenenemos una  una imagen de entrada de 300 x 300 a color (RGB) y queremos usar una red densa. Si la primera capa oculta tiene 100 neuronas, ¿Cuántos parámetros tendrá esa capa (sin incluir el bias) ?\n"
      ],
      "id": "present-consortium"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "novel-calcium"
      },
      "source": [
        ""
      ],
      "id": "novel-calcium"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complicated-positive"
      },
      "source": [
        "<a name='2.6'></a>\n",
        "## Cuestión 6   Ponga  las verdaderas ventajas de las redes convolucionales respecto a las densas\n",
        "\n",
        "**a)** Reducen el número total de parámetros, reduciendo así el overfitting.\n",
        "\n",
        "**b)** Permiten utilizar una misma 'función'  en varias localizaciones de la imagen de entrada, en lugar de aprender una función diferente para cada pixel.\n",
        "\n",
        "**c)** Permiten el uso del transfer learning.\n",
        "\n",
        "**d)** Generalmente son menos profundas, lo que facilita su entrenamiento.\n",
        "\n"
      ],
      "id": "complicated-positive"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dirty-nirvana"
      },
      "source": [
        "Las 4 son ventajas de las redes convolucionales respecto a las densas:\n",
        "\n",
        "a) Reducen el número total de parámetros, reduciendo así el overfitting\n",
        "\n",
        "b) Permiten utilizar una misma \"función\" en varias localizaciones de la imagen de entrada, en lugar de aprender una función diferente para cada pixel\n",
        "\n",
        "c) Permite el uso del transfer learning\n",
        "\n",
        "d) Generalmente son menos profundas, lo que facilita su entrenamiento"
      ],
      "id": "dirty-nirvana"
    }
  ]
}